{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3f28fc7-d3f7-43fb-a93f-674a5a4b5eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this utility, we have developed a highly reusable and configurable framework for managing data connections and orchestrating data ingestion workflows across various data sources and targets. By leveraging Databricks widgets, users can dynamically specify connection parameters, authentication details, and target destinations without modifying the underlying code. This approach enables seamless integration with multiple databases, cloud storage solutions (such as abfss, s3, dbfs), and supports flexible schema and table management.\n",
    "\n",
    "Key achievements and features:\n",
    "- **Parameterization**: All critical connection and ingestion parameters are exposed as widgets, allowing users to easily adapt the workflow to new sources, targets, and environments.\n",
    "- **Modularity**: The design supports both single-table and multi-table ingestion scenarios, with options for schema, table, and path customization.\n",
    "- **Security**: Sensitive information such as usernames and passwords are handled securely through widgets, minimizing hardcoding and exposure.\n",
    "- **Cloud Agnostic**: The framework supports ingestion to Databricks, Azure Data Lake (abfss), Amazon S3, and DBFS, making it suitable for hybrid and multi-cloud architectures.\n",
    "- **Reusability**: The utility can be reused across projects and teams by simply updating widget values, reducing development time and promoting best practices.\n",
    "- **Automation Ready**: With support for cron expressions and parameter-driven execution, the solution is ready for scheduling and automation in production pipelines.\n",
    "\n",
    "This utility empowers data engineers and analysts to rapidly onboard new data sources, standardize ingestion processes, and accelerate data-driven initiatives with minimal code changes and maximum flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7de1d1f7-0463-4809-b837-72a281a9d43d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Parameter Descriptions**\n",
    "\n",
    "The following parameters are used to configure the connection and ingestion process:\n",
    "\n",
    "- connection_id: Unique identifier for the connection configuration.\n",
    "- type: Type of the source database (e.g., postgres, mysql, etc.).\n",
    "- host: Hostname or IP address of the source database server.\n",
    "- port: Port number for the source database connection (integer).\n",
    "- database: Name of the source database.\n",
    "- schema: Schema name within the source database.\n",
    "- username: Username for authenticating to the source database.\n",
    "- password: Password for authenticating to the source database.\n",
    "- options: Additional connection options in key-value format (optional).\n",
    "- table_id: Specific table ID to process (optional, for single table operations).\n",
    "- cron_expression: Cron expression for scheduling ingestion jobs (optional).\n",
    "- schema_name: Name of the schema to be ingested or processed.\n",
    "- target_db: Target database in Databricks, abfss, s3, or dbfs where data will be stored.\n",
    "- target_prefix: Prefix to add to target table names (optional).\n",
    "- target_suffix: Suffix to add to target table names (optional).\n",
    "- target_path_type: Type of target path (optional, e.g., managed, external, abfss, s3, dbfs).\n",
    "- target_container: Target container name for abfss storage (required for abfss).\n",
    "- target_account: Target account name for abfss storage (required for abfss).\n",
    "- target_bucket: Target bucket name for s3 storage (required for s3).\n",
    "- target_mount: Target mount point for dbfs storage (required for dbfs).\n",
    "\n",
    "**Target Path Usage**\n",
    "\n",
    "- Default (local mount or anything else): The system will default to a general path under /mnt/datalake.\n",
    "  - You must provide:\n",
    "    - tgt_tbl_name: The target table or file name.\n",
    "    - (Optional) target_db: Used as a folder inside /mnt/datalake.\n",
    "  - Resulting path example: /mnt/datalake/<target_db>/<tgt_tbl_name>\n",
    "\n",
    "- abfss (Azure Data Lake Gen2): Used for writing to Azure Storage accounts with hierarchical namespaces.\n",
    "  - You must provide:\n",
    "    - target_container: The container name inside your Azure storage account.\n",
    "    - target_account: The Azure storage account name.\n",
    "    - tgt_tbl_name: The target table or file name.\n",
    "    - (Optional) target_db: Used as a folder inside the container.\n",
    "  - Resulting path example: abfss://<target_container>@<target_account>.dfs.core.windows.net/<target_db>/<tgt_tbl_name>\n",
    "\n",
    "- s3 (Amazon S3): Used for writing to AWS S3 buckets.\n",
    "  - You must provide:\n",
    "    - target_bucket: Your S3 bucket name.\n",
    "    - tgt_tbl_name: The target table or file name.\n",
    "    - (Optional) target_db: Used as a folder in the bucket.\n",
    "  - Resulting path example: s3://<target_bucket>/<target_db>/<tgt_tbl_name>\n",
    "\n",
    "- dbfs (Databricks File System): Used to write files under a mounted path in Databricks File System.\n",
    "  - You must provide:\n",
    "    - target_mount: The DBFS mount point.\n",
    "    - tgt_tbl_name: The target table or file name.\n",
    "    - (Optional) target_db: Used as a folder inside the mount.\n",
    "  - Resulting path example: /dbfs/mnt/<target_mount>/<target_db>/<tgt_tbl_name>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90b1eb5-9e4c-4d55-bf6c-cc631fe15d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Notebook Overview**\n",
    "This notebook is designed to manage and orchestrate metadata and ingestion workflows for various data connections in a structured and automated way. It performs the following key functions:\n",
    "\n",
    "**Widget Setup**\n",
    "Captures input parameters such as connection type, host, port, database, schema, username, password, optional settings, and a cron expression using Databricks widgets.\n",
    "\n",
    "**Parameter Retrieval**\n",
    "Reads the values provided via widgets for use in the logic that follows.\n",
    "\n",
    "**Connection Metadata Management**\n",
    "\n",
    "Checks for existing connection metadata in the workspace.default.connection_metadata table.\n",
    "\n",
    "If found, it loads the existing parameters.\n",
    "\n",
    "If not, it inserts the new connection metadata into the table.\n",
    "\n",
    "**Metadata Table Initialization**\n",
    "Calls a supporting notebook to ensure that required metadata tables are created.\n",
    "\n",
    "**Table Metadata Population**\n",
    "If no table metadata is available for the connection, it triggers a process to populate it.\n",
    "\n",
    "**Column Metadata Aggregation**\n",
    "Fetches and summarizes table and column metadata, calculating and displaying the number of columns per table.\n",
    "\n",
    "**Scheduled Ingestion Setup**\n",
    "If a cron expression is provided, sets up a scheduled ingestion job for the specified connection.\n",
    "\n",
    "**Immediate Ingestion Trigger**\n",
    "Initiates an immediate data ingestion run for the connection.\n",
    "\n",
    "The notebook includes conditional logic to prevent redundant metadata creation or updates, and allows ingestion to be triggered based on user-defined options (either scheduled or immediate).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef8cff08-0194-407c-87f8-d0848a2a3cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Database driver requirements for common databases:\n",
    "\n",
    " **MySQL:**\n",
    "\n",
    "   Install JDBC driver JAR (e.g., mysql-connector-java-8.0.33.jar)\n",
    "   %pip install mysql-connector-python\n",
    "\n",
    "**MariaDB:**\n",
    "\n",
    "   Install JDBC driver JAR (e.g., mariadb-java-client-3.3.2.jar)\n",
    "   %pip install mariadb\n",
    "\n",
    " **Oracle:**\n",
    "\n",
    "   Install JDBC driver JAR (e.g., ojdbc8.jar)\n",
    "   %pip install cx_Oracle\n",
    "\n",
    " **SQL Server:**\n",
    "\n",
    "   Install JDBC driver JAR (e.g., mssql-jdbc-12.4.2.jre8.jar)\n",
    "   %pip install pyodbc\n",
    "\n",
    " **PostgreSQL:**\n",
    "\n",
    "  %pip install psycopg2-binary\n",
    "\n",
    " **Snowflake:**\n",
    "\n",
    "   %pip install snowflake-connector-python\n",
    "\n",
    " **BigQuery:**\n",
    "\n",
    "   %pip install google-cloud-bigquery\n",
    "\n",
    " **Redshift:**\n",
    "\n",
    "   %pip install redshift-connector\n",
    "\n",
    " **IBM DB2:**\n",
    "\n",
    "   %pip install ibm-db\n",
    "\n",
    " **SAP HANA:**\n",
    " \n",
    "  %pip install hdbcli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874b09fc-7150-468f-992e-7466b92dc1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79693c8-4cde-42f3-9799-4dd5f072b835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"connection_id\", \"\", \"Connection ID\")\n",
    "dbutils.widgets.text(\"type\", \"\")\n",
    "dbutils.widgets.text(\"host\", \"\")\n",
    "dbutils.widgets.text(\"port\", \"\", \"Port (integer)\")\n",
    "dbutils.widgets.text(\"database\", \"\")\n",
    "dbutils.widgets.text(\"schema\", \"\")\n",
    "dbutils.widgets.text(\"username\", \"\")\n",
    "dbutils.widgets.text(\"password\", \"\")\n",
    "dbutils.widgets.text(\"options\", \"\")\n",
    "dbutils.widgets.text(\"table_id\", \"\", \"Table ID (optional, for single table)\")\n",
    "dbutils.widgets.text(\"cron_expression\", \"\")\n",
    "\n",
    "dbutils.widgets.text(\"schema_name\", \"\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"target_db\", \"\", \"Target Databricks,abfss,s3,dbfs Database\")\n",
    "dbutils.widgets.text(\"target_prefix\", \"\", \"Target Table Prefix (optional)\")\n",
    "dbutils.widgets.text(\"target_suffix\", \"\", \"Target Table Suffix (optional)\")\n",
    "dbutils.widgets.text(\"target_path_type\", \"\", \"Target Path Type (optional)\")\n",
    "dbutils.widgets.text(\"target_container\", \"\", \"Target Container (for abfss)\")\n",
    "dbutils.widgets.text(\"target_account\", \"\", \"Target Account (for abfss)\")\n",
    "dbutils.widgets.text(\"target_bucket\", \"\", \"Target Bucket (for s3)\")\n",
    "dbutils.widgets.text(\"target_mount\", \"\", \"Target Mount (for dbfs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c18529b-66a0-4f57-a4ba-515871b187e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connection_id = dbutils.widgets.get(\"connection_id\")\n",
    "type_ = dbutils.widgets.get(\"type\")\n",
    "host = dbutils.widgets.get(\"host\")\n",
    "port_str = dbutils.widgets.get(\"port\")\n",
    "port = int(port_str) if port_str else 5432\n",
    "database = dbutils.widgets.get(\"database\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "username = dbutils.widgets.get(\"username\")\n",
    "password = dbutils.widgets.get(\"password\")\n",
    "options = dbutils.widgets.get(\"options\")\n",
    "cron_expression = dbutils.widgets.get(\"cron_expression\")\n",
    "\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "target_db = dbutils.widgets.get(\"target_db\")\n",
    "target_prefix = dbutils.widgets.get(\"target_prefix\")\n",
    "target_suffix = dbutils.widgets.get(\"target_suffix\")\n",
    "target_path_type = dbutils.widgets.get(\"target_path_type\")\n",
    "target_container = dbutils.widgets.get(\"target_container\")\n",
    "target_account = dbutils.widgets.get(\"target_account\")\n",
    "target_bucket = dbutils.widgets.get(\"target_bucket\")\n",
    "target_mount = dbutils.widgets.get(\"target_mount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b92f46a-6256-4d87-bc3e-0f18ba6b92eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MySQL, you must install the JDBC driver JAR (e.g., mysql-connector-java-8.0.33.jar) and also run: %pip install mysql-connector-python\nCollecting mysql-connector-python\n  Downloading mysql_connector_python-9.4.0-cp311-cp311-manylinux_2_28_aarch64.whl.metadata (7.3 kB)\nDownloading mysql_connector_python-9.4.0-cp311-cp311-manylinux_2_28_aarch64.whl (33.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/33.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m30.4/33.5 MB\u001B[0m \u001B[31m174.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m33.5/33.5 MB\u001B[0m \u001B[31m121.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: mysql-connector-python\nSuccessfully installed mysql-connector-python-9.4.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "def print_driver_requirements(db_type):\n",
    "    db_type = db_type.lower()\n",
    "    if db_type == \"mysql\":\n",
    "        print(\"For MySQL, you must install the JDBC driver JAR (e.g., mysql-connector-java-8.0.33.jar) and also run: %pip install mysql-connector-python\")\n",
    "        %pip install mysql-connector-python\n",
    "    elif db_type == \"mariadb\":\n",
    "        print(\"For MariaDB, you must install the JDBC driver JAR (e.g., mariadb-java-client-3.3.2.jar) and also run: %pip install mariadb\")\n",
    "        %pip install mariadb\n",
    "    elif db_type == \"oracle\":\n",
    "        print(\"For Oracle, you must install the JDBC driver JAR (e.g., ojdbc8.jar) and also run: %pip install cx_Oracle\")\n",
    "        %pip install cx_Oracle\n",
    "    elif db_type == \"sqlserver\":\n",
    "        print(\"For SQL Server, you must install the JDBC driver JAR (e.g., mssql-jdbc-12.4.2.jre8.jar) and also run: %pip install pyodbc\")\n",
    "        %pip install pyodbc\n",
    "    elif db_type == \"postgresql\":\n",
    "        print(\"For PostgreSQL, you only need: %pip install psycopg2-binary\")\n",
    "        %pip install psycopg2-binary\n",
    "    elif db_type == \"snowflake\":\n",
    "        print(\"For Snowflake, you only need: %pip install snowflake-connector-python\")\n",
    "        %pip install snowflake-connector-python\n",
    "    elif db_type == \"bigquery\":\n",
    "        print(\"For BigQuery, you only need: %pip install google-cloud-bigquery\")\n",
    "        %pip install google-cloud-bigquery\n",
    "    elif db_type == \"redshift\":\n",
    "        print(\"For Redshift, you only need: %pip install redshift-connector\")\n",
    "        %pip install redshift-connector\n",
    "    elif db_type == \"db2\":\n",
    "        print(\"For IBM DB2, you only need: %pip install ibm-db\")\n",
    "        %pip install ibm-db\n",
    "    elif db_type == \"hana\":\n",
    "        print(\"For SAP HANA, you only need: %pip install hdbcli\")\n",
    "        %pip install hdbcli\n",
    "    else:\n",
    "        print(f\"Unsupported RDBMS type: {db_type}\")\n",
    "\n",
    "print_driver_requirements(type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b746df44-69be-45b4-8d83-c50d216d2884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "existing_df = spark.table(\"workspace.default.connection_metadata\").filter(f\"connection_id = '{connection_id}'\")\n",
    "if existing_df.count() > 0:\n",
    "    row = existing_df.collect()[0]\n",
    "    type_ = row['type']\n",
    "    host = row['host']\n",
    "    port = row['port']\n",
    "    database = row['database']\n",
    "    schema = row['schema']\n",
    "    username = row['username']\n",
    "    password = row['password']\n",
    "    options = row['options']\n",
    "    print(\"connection already exist as \"+connection_id)\n",
    "else:\n",
    "    from pyspark.sql import Row\n",
    "\n",
    "    new_row = Row(\n",
    "        connection_id=connection_id,\n",
    "        type=type_,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        schema=schema,\n",
    "        username=username,\n",
    "        password=password,\n",
    "        options=options\n",
    "    )\n",
    "    schema = StructType([\n",
    "    StructField(\"connection_id\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"host\", StringType(), True),\n",
    "    StructField(\"port\", IntegerType(), True),  # Set type explicitly\n",
    "    StructField(\"database\", StringType(), True),\n",
    "    StructField(\"schema\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"password\", StringType(), True),\n",
    "    StructField(\"options\", StringType(), True)\n",
    "])\n",
    "    new_df = spark.createDataFrame([new_row],schema)\n",
    "    #new_df.printSchema()\n",
    "    #spark.table(\"workspace.default.connection_metadata\").printSchema()\n",
    "    new_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"workspace.default.connection_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e8739e-3442-4270-a9d5-9b7f8f34484a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = dbutils.notebook.run(\"metadata_tables_ddl\", 600)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb721ac7-d711-46f1-b1b2-16ac28161279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('connection_id', StringType(), True), StructField('type', StringType(), True), StructField('host', StringType(), True), StructField('port', IntegerType(), True), StructField('database', StringType(), True), StructField('schema', StringType(), True), StructField('username', StringType(), True), StructField('password', StringType(), True), StructField('options', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54162678-a0d8-4778-b7f7-a45e0971a204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8168839338265294>, line 14\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT 1 FROM workspace.default.table_metadata WHERE connection_id = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconnection_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m LIMIT 1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m      2\u001B[0m     params \u001B[38;5;241m=\u001B[39m {\n",
       "\u001B[1;32m      3\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconnection_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: connection_id,\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mschema_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: schema_name,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget_mount\u001B[39m\u001B[38;5;124m\"\u001B[39m: target_mount\n",
       "\u001B[1;32m     13\u001B[0m     }\n",
       "\u001B[0;32m---> 14\u001B[0m     result \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mnotebook\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata_population_orchestration\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m600\u001B[39m, params)\n",
       "\u001B[1;32m     15\u001B[0m     display(result)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/dbutils.py:294\u001B[0m, in \u001B[0;36mDBUtils.NotebookHandler.run\u001B[0;34m(self, path, timeout_seconds, arguments, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    292\u001B[0m arguments_scala_map: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]\n",
       "\u001B[1;32m    293\u001B[0m arguments_scala_map \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoScalaMap(arguments)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
       "\u001B[0;32m--> 294\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetDbutils()\u001B[38;5;241m.\u001B[39mnotebook()\u001B[38;5;241m.\u001B[39mrun(\n",
       "\u001B[1;32m    295\u001B[0m     path, timeout_seconds, arguments_scala_map, __databricks_internal_cluster_spec)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n",
       "\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
       "\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o840.run.\n",
       ": com.databricks.WorkflowException: com.databricks.NotebookExecutionException: FAILED: Workload failed, see run output for details\n",
       "\tat com.databricks.workflow.WorkflowDriver.run(WorkflowDriver.scala:98)\n",
       "\tat com.databricks.dbutils_v1.impl.NotebookUtilsImpl.run(NotebookUtilsImpl.scala:130)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "Caused by: com.databricks.NotebookExecutionException: FAILED: Workload failed, see run output for details\n",
       "\tat com.databricks.workflow.WorkflowDriver.run0(WorkflowDriver.scala:146)\n",
       "\tat com.databricks.workflow.WorkflowDriver.run(WorkflowDriver.scala:93)\n",
       "\t... 13 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o840.run.\n: com.databricks.WorkflowException: com.databricks.NotebookExecutionException: FAILED: Workload failed, see run output for details\n\tat com.databricks.workflow.WorkflowDriver.run(WorkflowDriver.scala:98)\n\tat com.databricks.dbutils_v1.impl.NotebookUtilsImpl.run(NotebookUtilsImpl.scala:130)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.databricks.NotebookExecutionException: FAILED: Workload failed, see run output for details\n\tat com.databricks.workflow.WorkflowDriver.run0(WorkflowDriver.scala:146)\n\tat com.databricks.workflow.WorkflowDriver.run(WorkflowDriver.scala:93)\n\t... 13 more\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o840.run.\n: com.databricks.WorkflowException: com.databricks.NotebookExecutionException: FAILED: Workload failed, see run output for details\n\tat com.databricks.workflow.WorkflowDriver.run(WorkflowDriver.scala:98)\n\tat com.databricks.dbutils_v1.impl.NotebookUtilsImpl.run(NotebookUtilsImpl.scala:130)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.databricks.NotebookExecutionException: FAILED: Workload failed, see run output for details\n\tat com.databricks.workflow.WorkflowDriver.run0(WorkflowDriver.scala:146)\n\tat com.databricks.workflow.WorkflowDriver.run(WorkflowDriver.scala:93)\n\t... 13 more\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-8168839338265294>, line 14\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT 1 FROM workspace.default.table_metadata WHERE connection_id = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconnection_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m LIMIT 1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m      2\u001B[0m     params \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconnection_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: connection_id,\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mschema_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: schema_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget_mount\u001B[39m\u001B[38;5;124m\"\u001B[39m: target_mount\n\u001B[1;32m     13\u001B[0m     }\n\u001B[0;32m---> 14\u001B[0m     result \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mnotebook\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata_population_orchestration\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m600\u001B[39m, params)\n\u001B[1;32m     15\u001B[0m     display(result)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/dbutils.py:294\u001B[0m, in \u001B[0;36mDBUtils.NotebookHandler.run\u001B[0;34m(self, path, timeout_seconds, arguments, *args, **kwargs)\u001B[0m\n\u001B[1;32m    292\u001B[0m arguments_scala_map: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]\n\u001B[1;32m    293\u001B[0m arguments_scala_map \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoScalaMap(arguments)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m--> 294\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetDbutils()\u001B[38;5;241m.\u001B[39mnotebook()\u001B[38;5;241m.\u001B[39mrun(\n\u001B[1;32m    295\u001B[0m     path, timeout_seconds, arguments_scala_map, __databricks_internal_cluster_spec)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o840.run.\n: com.databricks.WorkflowException: com.databricks.NotebookExecutionException: FAILED: Workload failed, see run output for details\n\tat com.databricks.workflow.WorkflowDriver.run(WorkflowDriver.scala:98)\n\tat com.databricks.dbutils_v1.impl.NotebookUtilsImpl.run(NotebookUtilsImpl.scala:130)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.databricks.NotebookExecutionException: FAILED: Workload failed, see run output for details\n\tat com.databricks.workflow.WorkflowDriver.run0(WorkflowDriver.scala:146)\n\tat com.databricks.workflow.WorkflowDriver.run(WorkflowDriver.scala:93)\n\t... 13 more\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if spark.sql(f\"SELECT 1 FROM workspace.default.table_metadata WHERE connection_id = '{connection_id}' LIMIT 1\").count() == 0:\n",
    "    params = {\n",
    "        \"connection_id\": connection_id,\n",
    "    \"schema_name\": schema_name,\n",
    "    \"target_db\": target_db,\n",
    "    \"target_prefix\": target_prefix,\n",
    "    \"target_suffix\": target_suffix,\n",
    "    \"target_path_type\": target_path_type,\n",
    "    \"target_container\": target_container,\n",
    "    \"target_account\": target_account,\n",
    "    \"target_bucket\": target_bucket,\n",
    "    \"target_mount\": target_mount\n",
    "    }\n",
    "    result = dbutils.notebook.run(\"metadata_population_orchestration\", 600, params)\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48993cce-d7fd-4710-89e0-3daec93d9d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>table_name</th><th>count</th></tr></thead><tbody><tr><td>Album</td><td>3</td></tr><tr><td>Artist</td><td>2</td></tr><tr><td>Customer</td><td>13</td></tr><tr><td>Employee</td><td>15</td></tr><tr><td>Genre</td><td>2</td></tr><tr><td>Invoice</td><td>9</td></tr><tr><td>InvoiceLine</td><td>5</td></tr><tr><td>MediaType</td><td>2</td></tr><tr><td>Playlist</td><td>2</td></tr><tr><td>PlaylistTrack</td><td>2</td></tr><tr><td>Track</td><td>9</td></tr><tr><td>lego_colors</td><td>4</td></tr><tr><td>lego_inventories</td><td>3</td></tr><tr><td>lego_inventory_parts</td><td>5</td></tr><tr><td>lego_inventory_sets</td><td>3</td></tr><tr><td>lego_part_categories</td><td>2</td></tr><tr><td>lego_parts</td><td>3</td></tr><tr><td>lego_sets</td><td>5</td></tr><tr><td>lego_themes</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Album",
         3
        ],
        [
         "Artist",
         2
        ],
        [
         "Customer",
         13
        ],
        [
         "Employee",
         15
        ],
        [
         "Genre",
         2
        ],
        [
         "Invoice",
         9
        ],
        [
         "InvoiceLine",
         5
        ],
        [
         "MediaType",
         2
        ],
        [
         "Playlist",
         2
        ],
        [
         "PlaylistTrack",
         2
        ],
        [
         "Track",
         9
        ],
        [
         "lego_colors",
         4
        ],
        [
         "lego_inventories",
         3
        ],
        [
         "lego_inventory_parts",
         5
        ],
        [
         "lego_inventory_sets",
         3
        ],
        [
         "lego_part_categories",
         2
        ],
        [
         "lego_parts",
         3
        ],
        [
         "lego_sets",
         5
        ],
        [
         "lego_themes",
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_table_metadata = spark.sql(f\"SELECT * FROM workspace.default.table_metadata WHERE connection_id = '{connection_id}'\")\n",
    "#display(df_table_metadata)\n",
    "\n",
    "table_ids = [row.table_id for row in df_table_metadata.select(\"table_id\").distinct().collect()]\n",
    "table_ids_str = \",\".join([f\"'{tid}'\" for tid in table_ids])\n",
    "\n",
    "df_column_metadata = spark.sql(f\"SELECT * FROM workspace.default.column_metadata WHERE table_id IN ({table_ids_str})\")\n",
    "\n",
    "df_table_col_count = (\n",
    "    df_column_metadata.groupBy(\"table_id\")\n",
    "    .count()\n",
    "    .join(df_table_metadata.select(\"table_id\", \"table_name\"), on=\"table_id\", how=\"left\")\n",
    "    .select(\"table_name\", \"count\")\n",
    "    .orderBy(\"table_name\")\n",
    ")\n",
    "\n",
    "display(df_table_col_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b12ab21b-fb30-421b-be8e-d2ba4a35f027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if cron_expression:\n",
    "    params = {\n",
    "        \"connection_id\": connection_id,\n",
    "        \"cron_expression\": cron_expression,\n",
    "        \"ingest_mode\": \"schedule\"\n",
    "    }\n",
    "    result = dbutils.notebook.run(\"ingestion_orchestration\", 600, params)\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8df6432-34e2-4947-986a-97ad78826fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c584ec4a-555d-4933-b0df-ccdef47ca76d\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "run_id = str(uuid.uuid4())\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd004f91-1996-4bc8-8fcd-563b39dcb613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31m_MultiThreadedRendezvous\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:172\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._has_next\u001B[0;34m(self, is_last)\u001B[0m\n",
       "\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 172\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_iter(\n",
       "\u001B[1;32m    173\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
       "\u001B[1;32m    174\u001B[0m     )\n",
       "\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:297\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._call_iter\u001B[0;34m(self, iter_fun)\u001B[0m\n",
       "\u001B[1;32m    296\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 297\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    299\u001B[0m     \u001B[38;5;66;03m# Remove the iterator, so that a new one will be created after retry.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:277\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._call_iter\u001B[0;34m(self, iter_fun)\u001B[0m\n",
       "\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m iter_fun()\n",
       "\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m grpc\u001B[38;5;241m.\u001B[39mRpcError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:173\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._has_next.<locals>.<lambda>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    172\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_iter(\n",
       "\u001B[0;32m--> 173\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
       "\u001B[1;32m    174\u001B[0m     )\n",
       "\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:543\u001B[0m, in \u001B[0;36m_Rendezvous.__next__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    542\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n",
       "\u001B[0;32m--> 543\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:952\u001B[0m, in \u001B[0;36m_MultiThreadedRendezvous._next\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    951\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 952\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
       "\u001B[1;32m    954\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_response_ready\u001B[39m():\n",
       "\n",
       "\u001B[0;31m_MultiThreadedRendezvous\u001B[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n",
       "\tstatus = StatusCode.UNAVAILABLE\n",
       "\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n",
       "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\", grpc_status:14, created_time:\"2025-08-01T11:30:13.723015359+00:00\"}\"\n",
       ">\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5014603495019467>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdate workspace.default.table_metadata \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mset onboarded_flag = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mN\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mwhere connection_id = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconn_postgres_chinook\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n",
       "\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n",
       "\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n",
       "\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n",
       "\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n",
       "\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n",
       "\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:147\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    143\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m    144\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124mql command because spark connect initialization failed and no stacktrace was available.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    145\u001B[0m     )\n",
       "\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_py4j:\n",
       "\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_py4j(cell)\n",
       "\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_sql_comm_handler(cell)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:231\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n",
       "\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    230\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
       "\u001B[0;32m--> 231\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    232\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:227\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n",
       "\u001B[1;32m    225\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecuting subquery: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stdout__)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
       "\u001B[1;32m    226\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n",
       "\u001B[0;32m--> 227\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    230\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:241\u001B[0m, in \u001B[0;36mSqlMagic.handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n",
       "\u001B[1;32m    239\u001B[0m \u001B[38;5;66;03m# This is consistent with the behavior of the execute_via_sql_comm_handler.\u001B[39;00m\n",
       "\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m directive_name \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDLTCommand\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m--> 241\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregister_udf(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mescapeWidgetValues(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()))\n",
       "\u001B[1;32m    243\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m directive_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCacheTableAs\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:116\u001B[0m, in \u001B[0;36mSqlMagic.register_udf\u001B[0;34m(self, escaped_widget_values)\u001B[0m\n",
       "\u001B[1;32m    113\u001B[0m     cases \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\n",
       "\u001B[1;32m    114\u001B[0m         [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m when \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m then \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k, v) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m escaped_widget_values\u001B[38;5;241m.\u001B[39mitems()])\n",
       "\u001B[1;32m    115\u001B[0m     function \u001B[38;5;241m=\u001B[39m comment \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m signature \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mreturn case key\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m cases \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124melse \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m error \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m--> 116\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masserting_spark\u001B[38;5;241m.\u001B[39msql(function)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:736\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    733\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n",
       "\u001B[1;32m    735\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n",
       "\u001B[0;32m--> 736\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n",
       "\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n",
       "\u001B[1;32m    738\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1297\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1295\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1296\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1297\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1298\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1299\u001B[0m )\n",
       "\u001B[1;32m   1300\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1301\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1752\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1755\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1756\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1757\u001B[0m     ):\n",
       "\u001B[1;32m   1758\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1759\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1729\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1730\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2049\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\u001B[1;32m   2050\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n",
       "\u001B[1;32m   2051\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n",
       "\u001B[1;32m   2052\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1717\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1708\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_reattachable_execute:\n",
       "\u001B[1;32m   1709\u001B[0m     \u001B[38;5;66;03m# Don't use retryHandler - own retry handling is inside.\u001B[39;00m\n",
       "\u001B[1;32m   1710\u001B[0m     generator \u001B[38;5;241m=\u001B[39m ExecutePlanResponseReattachableIterator(\n",
       "\u001B[1;32m   1711\u001B[0m         req,\n",
       "\u001B[1;32m   1712\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1715\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_release_until,\n",
       "\u001B[1;32m   1716\u001B[0m     )\n",
       "\u001B[0;32m-> 1717\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m generator:\n",
       "\u001B[1;32m   1718\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m handle_response(b)\n",
       "\u001B[1;32m   1719\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m<frozen _collections_abc>:330\u001B[0m, in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:139\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator.send\u001B[0;34m(self, value)\u001B[0m\n",
       "\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend\u001B[39m(\u001B[38;5;28mself\u001B[39m, value: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pb2\u001B[38;5;241m.\u001B[39mExecutePlanResponse:\n",
       "\u001B[1;32m    138\u001B[0m     \u001B[38;5;66;03m# will trigger reattach in case the stream completed without result_complete\u001B[39;00m\n",
       "\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_next():\n",
       "\u001B[1;32m    140\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m()\n",
       "\u001B[1;32m    142\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:200\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._has_next\u001B[0;34m(self, is_last)\u001B[0m\n",
       "\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_release_all()\n",
       "\u001B[0;32m--> 200\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:168\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._has_next\u001B[0;34m(self, is_last)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m result_complete \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result_complete \u001B[38;5;129;01mor\u001B[39;00m is_last\n",
       "\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 168\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n",
       "\u001B[1;32m    169\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n",
       "\u001B[1;32m    170\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n",
       "\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n",
       "\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n",
       "\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RetriesExceeded",
        "evalue": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "metadata": {
        "errorSummary": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "RETRIES_EXCEEDED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31m_MultiThreadedRendezvous\u001B[0m                  Traceback (most recent call last)",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:172\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._has_next\u001B[0;34m(self, is_last)\u001B[0m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 172\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_iter(\n\u001B[1;32m    173\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    174\u001B[0m     )\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:297\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._call_iter\u001B[0;34m(self, iter_fun)\u001B[0m\n\u001B[1;32m    296\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 297\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;66;03m# Remove the iterator, so that a new one will be created after retry.\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:277\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._call_iter\u001B[0;34m(self, iter_fun)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m iter_fun()\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m grpc\u001B[38;5;241m.\u001B[39mRpcError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:173\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._has_next.<locals>.<lambda>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    172\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_iter(\n\u001B[0;32m--> 173\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    174\u001B[0m     )\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:543\u001B[0m, in \u001B[0;36m_Rendezvous.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 543\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:952\u001B[0m, in \u001B[0;36m_MultiThreadedRendezvous._next\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    951\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 952\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m    954\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_response_ready\u001B[39m():\n",
        "\u001B[0;31m_MultiThreadedRendezvous\u001B[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\", grpc_status:14, created_time:\"2025-08-01T11:30:13.723015359+00:00\"}\"\n>",
        "\nThe above exception was the direct cause of the following exception:\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)",
        "File \u001B[0;32m<command-5014603495019467>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdate workspace.default.table_metadata \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mset onboarded_flag = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mN\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mwhere connection_id = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconn_postgres_chinook\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:147\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m    144\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124mql command because spark connect initialization failed and no stacktrace was available.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    145\u001B[0m     )\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_py4j:\n\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_py4j(cell)\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_sql_comm_handler(cell)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:231\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    230\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n\u001B[0;32m--> 231\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:227\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n\u001B[1;32m    225\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecuting subquery: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stdout__)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    226\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n\u001B[0;32m--> 227\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    230\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:241\u001B[0m, in \u001B[0;36mSqlMagic.handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;66;03m# This is consistent with the behavior of the execute_via_sql_comm_handler.\u001B[39;00m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m directive_name \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDLTCommand\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 241\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregister_udf(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mescapeWidgetValues(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()))\n\u001B[1;32m    243\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m directive_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCacheTableAs\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:116\u001B[0m, in \u001B[0;36mSqlMagic.register_udf\u001B[0;34m(self, escaped_widget_values)\u001B[0m\n\u001B[1;32m    113\u001B[0m     cases \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m    114\u001B[0m         [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m when \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m then \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k, v) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m escaped_widget_values\u001B[38;5;241m.\u001B[39mitems()])\n\u001B[1;32m    115\u001B[0m     function \u001B[38;5;241m=\u001B[39m comment \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m signature \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mreturn case key\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m cases \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124melse \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m error \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 116\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masserting_spark\u001B[38;5;241m.\u001B[39msql(function)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:736\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m    733\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n\u001B[1;32m    735\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n\u001B[0;32m--> 736\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m    738\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1297\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1295\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1296\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1297\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1298\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1299\u001B[0m )\n\u001B[1;32m   1300\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1301\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1752\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1755\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1756\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1757\u001B[0m     ):\n\u001B[1;32m   1758\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1759\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1729\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1730\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2049\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n\u001B[1;32m   2050\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n\u001B[1;32m   2051\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m   2052\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1717\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1708\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_reattachable_execute:\n\u001B[1;32m   1709\u001B[0m     \u001B[38;5;66;03m# Don't use retryHandler - own retry handling is inside.\u001B[39;00m\n\u001B[1;32m   1710\u001B[0m     generator \u001B[38;5;241m=\u001B[39m ExecutePlanResponseReattachableIterator(\n\u001B[1;32m   1711\u001B[0m         req,\n\u001B[1;32m   1712\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1715\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_release_until,\n\u001B[1;32m   1716\u001B[0m     )\n\u001B[0;32m-> 1717\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m generator:\n\u001B[1;32m   1718\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m handle_response(b)\n\u001B[1;32m   1719\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "File \u001B[0;32m<frozen _collections_abc>:330\u001B[0m, in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:139\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator.send\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend\u001B[39m(\u001B[38;5;28mself\u001B[39m, value: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pb2\u001B[38;5;241m.\u001B[39mExecutePlanResponse:\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;66;03m# will trigger reattach in case the stream completed without result_complete\u001B[39;00m\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_next():\n\u001B[1;32m    140\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m()\n\u001B[1;32m    142\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:200\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._has_next\u001B[0;34m(self, is_last)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_release_all()\n\u001B[0;32m--> 200\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:168\u001B[0m, in \u001B[0;36mExecutePlanResponseReattachableIterator._has_next\u001B[0;34m(self, is_last)\u001B[0m\n\u001B[1;32m    166\u001B[0m result_complete \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result_complete \u001B[38;5;129;01mor\u001B[39;00m is_last\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 168\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n\u001B[1;32m    169\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[1;32m    170\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "/*update workspace.default.table_metadata \n",
    "set onboarded_flag = 'N' \n",
    "where connection_id = 'conn_postgres_chinook'*/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf4866f-4a39-49e6-93ff-58036c34a33c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    \"connection_id\": connection_id\n",
    "    ,\"ingest_mode\": \"immediate\"\n",
    "    ,\"run_id\": run_id\n",
    "}\n",
    "\n",
    "result = dbutils.notebook.run(\"ingestion_orchestration\", 600, params)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b99050a-7f74-4f0e-b3ff-efc3b0211ab9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754042660386}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>transaction_id</th><th>run_id</th><th>connection_id</th><th>table_id</th><th>table_name</th><th>dateandtime</th><th>mode</th><th>status</th><th>error</th><th>target_table</th><th>number_of_rows</th><th>other_comments</th></tr></thead><tbody><tr><td>26f989e7-4a75-410e-8762-38df7b6fc0f3</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_inventory_sets</td><td>lego_inventory_sets</td><td>2025-08-04 11:48:56</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_lego_inventory_sets</td><td>0</td><td></td></tr><tr><td>26f989e7-4a75-410e-8762-38df7b6fc0f3</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_inventory_sets</td><td>lego_inventory_sets</td><td>2025-08-04 11:49:02</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_lego_inventory_sets</td><td>2846</td><td></td></tr><tr><td>61c9efa1-e227-42f1-9129-56f6eea5877f</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_part_categories</td><td>lego_part_categories</td><td>2025-08-04 11:49:04</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_lego_part_categories</td><td>0</td><td></td></tr><tr><td>61c9efa1-e227-42f1-9129-56f6eea5877f</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_part_categories</td><td>lego_part_categories</td><td>2025-08-04 11:49:08</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_lego_part_categories</td><td>57</td><td></td></tr><tr><td>265269dd-58cc-4d5d-bd7a-f49e0e96df34</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_colors</td><td>lego_colors</td><td>2025-08-04 11:49:10</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_lego_colors</td><td>0</td><td></td></tr><tr><td>265269dd-58cc-4d5d-bd7a-f49e0e96df34</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_colors</td><td>lego_colors</td><td>2025-08-04 11:49:14</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_lego_colors</td><td>135</td><td></td></tr><tr><td>bab28eb5-eabf-47ee-8257-8259d14f8b4b</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_inventories</td><td>lego_inventories</td><td>2025-08-04 11:49:16</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_lego_inventories</td><td>0</td><td></td></tr><tr><td>bab28eb5-eabf-47ee-8257-8259d14f8b4b</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_inventories</td><td>lego_inventories</td><td>2025-08-04 11:49:20</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_lego_inventories</td><td>11681</td><td></td></tr><tr><td>0b7ea3cf-0561-41d3-a365-f7eb77c576d3</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_inventory_parts</td><td>lego_inventory_parts</td><td>2025-08-04 11:49:21</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_lego_inventory_parts</td><td>0</td><td></td></tr><tr><td>0b7ea3cf-0561-41d3-a365-f7eb77c576d3</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_inventory_parts</td><td>lego_inventory_parts</td><td>2025-08-04 11:49:26</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_lego_inventory_parts</td><td>580251</td><td></td></tr><tr><td>5563d639-f63b-4221-8f3a-ca980d57fcbc</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Customer</td><td>Customer</td><td>2025-08-04 11:50:07</td><td>overwrite</td><td>ERROR</td><td>[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 743adf24-087f-4c58-b527-07259c4e8a55).\n",
       "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
       "'.option(\"mergeSchema\", \"true\")'.\n",
       "For other operations, set the session configuration\n",
       "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
       "specific to the operation for details.\n",
       "\n",
       "Table schema:\n",
       "root\n",
       "-- customer_id: integer (nullable = true)\n",
       "-- store_id: integer (nullable = true)\n",
       "-- first_name: string (nullable = true)\n",
       "-- last_name: string (nullable = true)\n",
       "-- email: string (nullable = true)\n",
       "-- address_id: integer (nullable = true)\n",
       "-- activebool: boolean (nullable = true)\n",
       "-- create_date: date (nullable = true)\n",
       "-- last_update: timestamp (nullable = true)\n",
       "-- active: integer (nullable = true)\n",
       "\n",
       "\n",
       "Data schema:\n",
       "root\n",
       "-- CustomerId: integer (nullable = true)\n",
       "-- FirstName: string (nullable = true)\n",
       "-- LastName: string (nullable = true)\n",
       "-- Company: string (nullable = true)\n",
       "-- Address: string (nullable = true)\n",
       "-- City: string (nullable = true)\n",
       "-- State: string (nullable = true)\n",
       "-- Country: string (nullable = true)\n",
       "-- PostalCode: string (nullable = true)\n",
       "-- Phone: string (nullable = true)\n",
       "-- Fax: string (nullable = true)\n",
       "-- Email: string (nullable = true)\n",
       "-- SupportRepId: integer (nullable = true)\n",
       "\n",
       "         \n",
       "To overwrite your schema or change partitioning, please set:\n",
       "'.option(\"overwriteSchema\", \"true\")'.\n",
       "\n",
       "Note that the schema can't be overwritten when using\n",
       "'replaceWhere'.\n",
       "         \n",
       "Table ACLs are enabled in this cluster, so automatic schema migration is not allowed. Please use the ALTER TABLE command for changing the schema.         \n",
       "\n",
       "JVM stacktrace:\n",
       "com.databricks.sql.transaction.tahoe.DeltaAnalysisException\n",
       "\tat com.databricks.sql.transaction.tahoe.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:4189)\n",
       "\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:231)\n",
       "\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:92)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.updateMetadata(WriteIntoDeltaEdge.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:314)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:210)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:625)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:709)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:393)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:255)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:374)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$9(CreateDeltaTableCommand.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)\n",
       "\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:57)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$5(CreateDeltaTableCommand.scala:293)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:91)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:91)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:91)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:91)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:254)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:646)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:268)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1669)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1652)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:314)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:826)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:826)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1632)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:813)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:830)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:805)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:266)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:339)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:461)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:460)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:523)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:443)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:817)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:372)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:372)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:846)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:371)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:237)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:770)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:456)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1331)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:452)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:381)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:450)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:517)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:509)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:509)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:509)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:337)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:342)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:545)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:815)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:664)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:571)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3649)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3081)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:140)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:620)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:620)</td><td>default.stg_Customer</td><td>0</td><td></td></tr><tr><td>e8a5a52c-ea0c-4c9a-83e2-ec8f00a97595</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_PlaylistTrack</td><td>PlaylistTrack</td><td>2025-08-04 11:49:54</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_PlaylistTrack</td><td>0</td><td></td></tr><tr><td>e8a5a52c-ea0c-4c9a-83e2-ec8f00a97595</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_PlaylistTrack</td><td>PlaylistTrack</td><td>2025-08-04 11:49:57</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_PlaylistTrack</td><td>8715</td><td></td></tr><tr><td>b5fb3941-7d5e-4741-96ec-6a50d3899215</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_themes</td><td>lego_themes</td><td>2025-08-04 11:49:40</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_lego_themes</td><td>0</td><td></td></tr><tr><td>c1599c0c-53c6-4a4f-a8d5-92a9f34536f1</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_InvoiceLine</td><td>InvoiceLine</td><td>2025-08-04 11:50:13</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_InvoiceLine</td><td>0</td><td></td></tr><tr><td>b5fb3941-7d5e-4741-96ec-6a50d3899215</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_themes</td><td>lego_themes</td><td>2025-08-04 11:49:44</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_lego_themes</td><td>614</td><td></td></tr><tr><td>c1599c0c-53c6-4a4f-a8d5-92a9f34536f1</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_InvoiceLine</td><td>InvoiceLine</td><td>2025-08-04 11:50:17</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_InvoiceLine</td><td>2240</td><td></td></tr><tr><td>d6650c39-2f3b-4fa0-9b30-3d3d43933980</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_parts</td><td>lego_parts</td><td>2025-08-04 11:49:28</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_lego_parts</td><td>0</td><td></td></tr><tr><td>d6650c39-2f3b-4fa0-9b30-3d3d43933980</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_parts</td><td>lego_parts</td><td>2025-08-04 11:49:32</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_lego_parts</td><td>25993</td><td></td></tr><tr><td>21e4cfa9-df39-4231-aea7-a64e14aabfdb</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_sets</td><td>lego_sets</td><td>2025-08-04 11:49:34</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_lego_sets</td><td>0</td><td></td></tr><tr><td>341a339c-6c6c-4441-974d-f99d27734ede</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_MediaType</td><td>MediaType</td><td>2025-08-04 11:50:28</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_MediaType</td><td>0</td><td></td></tr><tr><td>341a339c-6c6c-4441-974d-f99d27734ede</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_MediaType</td><td>MediaType</td><td>2025-08-04 11:50:32</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_MediaType</td><td>5</td><td></td></tr><tr><td>21e4cfa9-df39-4231-aea7-a64e14aabfdb</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_lego_sets</td><td>lego_sets</td><td>2025-08-04 11:49:38</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_lego_sets</td><td>11673</td><td></td></tr><tr><td>af6aceb8-8090-4b63-9cb6-fa2481e4c6ea</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Employee</td><td>Employee</td><td>2025-08-04 11:49:59</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_Employee</td><td>0</td><td></td></tr><tr><td>5563d639-f63b-4221-8f3a-ca980d57fcbc</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Customer</td><td>Customer</td><td>2025-08-04 11:50:04</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_Customer</td><td>0</td><td></td></tr><tr><td>7d67cd17-361a-4512-a6f2-24df54746649</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Playlist</td><td>Playlist</td><td>2025-08-04 11:49:46</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_Playlist</td><td>0</td><td></td></tr><tr><td>af6aceb8-8090-4b63-9cb6-fa2481e4c6ea</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Employee</td><td>Employee</td><td>2025-08-04 11:50:03</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_Employee</td><td>8</td><td></td></tr><tr><td>7d67cd17-361a-4512-a6f2-24df54746649</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Playlist</td><td>Playlist</td><td>2025-08-04 11:49:53</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_Playlist</td><td>18</td><td></td></tr><tr><td>07a54de8-f925-425b-8fb3-66af4d768dc0</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Invoice</td><td>Invoice</td><td>2025-08-04 11:50:08</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_Invoice</td><td>0</td><td></td></tr><tr><td>07a54de8-f925-425b-8fb3-66af4d768dc0</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Invoice</td><td>Invoice</td><td>2025-08-04 11:50:12</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_Invoice</td><td>412</td><td></td></tr><tr><td>317d1733-713c-41a7-a71d-3241d123aa46</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Artist</td><td>Artist</td><td>2025-08-04 11:50:33</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_Artist</td><td>0</td><td></td></tr><tr><td>317d1733-713c-41a7-a71d-3241d123aa46</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Artist</td><td>Artist</td><td>2025-08-04 11:50:37</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_Artist</td><td>275</td><td></td></tr><tr><td>00a1325f-235d-49f2-beb5-8039d6103dfa</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Album</td><td>Album</td><td>2025-08-04 11:50:38</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_Album</td><td>0</td><td></td></tr><tr><td>76a249f8-3682-4bb1-9d77-da96818cceb5</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Track</td><td>Track</td><td>2025-08-04 11:50:18</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_Track</td><td>0</td><td></td></tr><tr><td>91dd352f-0526-4d0b-ae3e-550765ab0f3c</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Genre</td><td>Genre</td><td>2025-08-04 11:50:24</td><td>overwrite</td><td>INPROGRESS</td><td>None</td><td>default.stg_Genre</td><td>0</td><td></td></tr><tr><td>76a249f8-3682-4bb1-9d77-da96818cceb5</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Track</td><td>Track</td><td>2025-08-04 11:50:22</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_Track</td><td>3503</td><td></td></tr><tr><td>00a1325f-235d-49f2-beb5-8039d6103dfa</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Album</td><td>Album</td><td>2025-08-04 11:50:42</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_Album</td><td>347</td><td></td></tr><tr><td>91dd352f-0526-4d0b-ae3e-550765ab0f3c</td><td>5c85f060-baa1-496b-a006-ea5bac4e00fe</td><td>conn_postgres_chinook</td><td>conn_postgres_chinook_Genre</td><td>Genre</td><td>2025-08-04 11:50:27</td><td>overwrite</td><td>COMPLETED</td><td>None</td><td>default.stg_Genre</td><td>25</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "26f989e7-4a75-410e-8762-38df7b6fc0f3",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_inventory_sets",
         "lego_inventory_sets",
         "2025-08-04 11:48:56",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_lego_inventory_sets",
         0,
         ""
        ],
        [
         "26f989e7-4a75-410e-8762-38df7b6fc0f3",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_inventory_sets",
         "lego_inventory_sets",
         "2025-08-04 11:49:02",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_lego_inventory_sets",
         2846,
         ""
        ],
        [
         "61c9efa1-e227-42f1-9129-56f6eea5877f",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_part_categories",
         "lego_part_categories",
         "2025-08-04 11:49:04",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_lego_part_categories",
         0,
         ""
        ],
        [
         "61c9efa1-e227-42f1-9129-56f6eea5877f",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_part_categories",
         "lego_part_categories",
         "2025-08-04 11:49:08",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_lego_part_categories",
         57,
         ""
        ],
        [
         "265269dd-58cc-4d5d-bd7a-f49e0e96df34",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_colors",
         "lego_colors",
         "2025-08-04 11:49:10",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_lego_colors",
         0,
         ""
        ],
        [
         "265269dd-58cc-4d5d-bd7a-f49e0e96df34",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_colors",
         "lego_colors",
         "2025-08-04 11:49:14",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_lego_colors",
         135,
         ""
        ],
        [
         "bab28eb5-eabf-47ee-8257-8259d14f8b4b",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_inventories",
         "lego_inventories",
         "2025-08-04 11:49:16",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_lego_inventories",
         0,
         ""
        ],
        [
         "bab28eb5-eabf-47ee-8257-8259d14f8b4b",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_inventories",
         "lego_inventories",
         "2025-08-04 11:49:20",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_lego_inventories",
         11681,
         ""
        ],
        [
         "0b7ea3cf-0561-41d3-a365-f7eb77c576d3",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_inventory_parts",
         "lego_inventory_parts",
         "2025-08-04 11:49:21",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_lego_inventory_parts",
         0,
         ""
        ],
        [
         "0b7ea3cf-0561-41d3-a365-f7eb77c576d3",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_inventory_parts",
         "lego_inventory_parts",
         "2025-08-04 11:49:26",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_lego_inventory_parts",
         580251,
         ""
        ],
        [
         "5563d639-f63b-4221-8f3a-ca980d57fcbc",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Customer",
         "Customer",
         "2025-08-04 11:50:07",
         "overwrite",
         "ERROR",
         "[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 743adf24-087f-4c58-b527-07259c4e8a55).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- customer_id: integer (nullable = true)\n-- store_id: integer (nullable = true)\n-- first_name: string (nullable = true)\n-- last_name: string (nullable = true)\n-- email: string (nullable = true)\n-- address_id: integer (nullable = true)\n-- activebool: boolean (nullable = true)\n-- create_date: date (nullable = true)\n-- last_update: timestamp (nullable = true)\n-- active: integer (nullable = true)\n\n\nData schema:\nroot\n-- CustomerId: integer (nullable = true)\n-- FirstName: string (nullable = true)\n-- LastName: string (nullable = true)\n-- Company: string (nullable = true)\n-- Address: string (nullable = true)\n-- City: string (nullable = true)\n-- State: string (nullable = true)\n-- Country: string (nullable = true)\n-- PostalCode: string (nullable = true)\n-- Phone: string (nullable = true)\n-- Fax: string (nullable = true)\n-- Email: string (nullable = true)\n-- SupportRepId: integer (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         \nTable ACLs are enabled in this cluster, so automatic schema migration is not allowed. Please use the ALTER TABLE command for changing the schema.         \n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.DeltaAnalysisException\n\tat com.databricks.sql.transaction.tahoe.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:4189)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:231)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:92)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.updateMetadata(WriteIntoDeltaEdge.scala:112)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:314)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:210)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:625)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:709)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:393)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:255)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:374)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$9(CreateDeltaTableCommand.scala:297)\n\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:57)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$5(CreateDeltaTableCommand.scala:293)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:91)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:91)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:91)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:91)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:254)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:646)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:142)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:268)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1669)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:142)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1652)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:314)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:826)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:73)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:826)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1632)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:813)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:830)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:805)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:266)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:339)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:461)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:461)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:460)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:523)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:443)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:817)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:372)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:372)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:846)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:371)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:237)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:770)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:456)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1331)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:452)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:381)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:450)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:517)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:509)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:509)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:509)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:337)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:342)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:545)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:815)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:664)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:571)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3649)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3081)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:140)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:620)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:620)",
         "default.stg_Customer",
         0,
         ""
        ],
        [
         "e8a5a52c-ea0c-4c9a-83e2-ec8f00a97595",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_PlaylistTrack",
         "PlaylistTrack",
         "2025-08-04 11:49:54",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_PlaylistTrack",
         0,
         ""
        ],
        [
         "e8a5a52c-ea0c-4c9a-83e2-ec8f00a97595",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_PlaylistTrack",
         "PlaylistTrack",
         "2025-08-04 11:49:57",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_PlaylistTrack",
         8715,
         ""
        ],
        [
         "b5fb3941-7d5e-4741-96ec-6a50d3899215",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_themes",
         "lego_themes",
         "2025-08-04 11:49:40",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_lego_themes",
         0,
         ""
        ],
        [
         "c1599c0c-53c6-4a4f-a8d5-92a9f34536f1",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_InvoiceLine",
         "InvoiceLine",
         "2025-08-04 11:50:13",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_InvoiceLine",
         0,
         ""
        ],
        [
         "b5fb3941-7d5e-4741-96ec-6a50d3899215",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_themes",
         "lego_themes",
         "2025-08-04 11:49:44",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_lego_themes",
         614,
         ""
        ],
        [
         "c1599c0c-53c6-4a4f-a8d5-92a9f34536f1",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_InvoiceLine",
         "InvoiceLine",
         "2025-08-04 11:50:17",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_InvoiceLine",
         2240,
         ""
        ],
        [
         "d6650c39-2f3b-4fa0-9b30-3d3d43933980",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_parts",
         "lego_parts",
         "2025-08-04 11:49:28",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_lego_parts",
         0,
         ""
        ],
        [
         "d6650c39-2f3b-4fa0-9b30-3d3d43933980",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_parts",
         "lego_parts",
         "2025-08-04 11:49:32",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_lego_parts",
         25993,
         ""
        ],
        [
         "21e4cfa9-df39-4231-aea7-a64e14aabfdb",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_sets",
         "lego_sets",
         "2025-08-04 11:49:34",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_lego_sets",
         0,
         ""
        ],
        [
         "341a339c-6c6c-4441-974d-f99d27734ede",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_MediaType",
         "MediaType",
         "2025-08-04 11:50:28",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_MediaType",
         0,
         ""
        ],
        [
         "341a339c-6c6c-4441-974d-f99d27734ede",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_MediaType",
         "MediaType",
         "2025-08-04 11:50:32",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_MediaType",
         5,
         ""
        ],
        [
         "21e4cfa9-df39-4231-aea7-a64e14aabfdb",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_lego_sets",
         "lego_sets",
         "2025-08-04 11:49:38",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_lego_sets",
         11673,
         ""
        ],
        [
         "af6aceb8-8090-4b63-9cb6-fa2481e4c6ea",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Employee",
         "Employee",
         "2025-08-04 11:49:59",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_Employee",
         0,
         ""
        ],
        [
         "5563d639-f63b-4221-8f3a-ca980d57fcbc",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Customer",
         "Customer",
         "2025-08-04 11:50:04",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_Customer",
         0,
         ""
        ],
        [
         "7d67cd17-361a-4512-a6f2-24df54746649",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Playlist",
         "Playlist",
         "2025-08-04 11:49:46",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_Playlist",
         0,
         ""
        ],
        [
         "af6aceb8-8090-4b63-9cb6-fa2481e4c6ea",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Employee",
         "Employee",
         "2025-08-04 11:50:03",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_Employee",
         8,
         ""
        ],
        [
         "7d67cd17-361a-4512-a6f2-24df54746649",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Playlist",
         "Playlist",
         "2025-08-04 11:49:53",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_Playlist",
         18,
         ""
        ],
        [
         "07a54de8-f925-425b-8fb3-66af4d768dc0",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Invoice",
         "Invoice",
         "2025-08-04 11:50:08",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_Invoice",
         0,
         ""
        ],
        [
         "07a54de8-f925-425b-8fb3-66af4d768dc0",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Invoice",
         "Invoice",
         "2025-08-04 11:50:12",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_Invoice",
         412,
         ""
        ],
        [
         "317d1733-713c-41a7-a71d-3241d123aa46",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Artist",
         "Artist",
         "2025-08-04 11:50:33",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_Artist",
         0,
         ""
        ],
        [
         "317d1733-713c-41a7-a71d-3241d123aa46",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Artist",
         "Artist",
         "2025-08-04 11:50:37",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_Artist",
         275,
         ""
        ],
        [
         "00a1325f-235d-49f2-beb5-8039d6103dfa",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Album",
         "Album",
         "2025-08-04 11:50:38",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_Album",
         0,
         ""
        ],
        [
         "76a249f8-3682-4bb1-9d77-da96818cceb5",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Track",
         "Track",
         "2025-08-04 11:50:18",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_Track",
         0,
         ""
        ],
        [
         "91dd352f-0526-4d0b-ae3e-550765ab0f3c",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Genre",
         "Genre",
         "2025-08-04 11:50:24",
         "overwrite",
         "INPROGRESS",
         "None",
         "default.stg_Genre",
         0,
         ""
        ],
        [
         "76a249f8-3682-4bb1-9d77-da96818cceb5",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Track",
         "Track",
         "2025-08-04 11:50:22",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_Track",
         3503,
         ""
        ],
        [
         "00a1325f-235d-49f2-beb5-8039d6103dfa",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Album",
         "Album",
         "2025-08-04 11:50:42",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_Album",
         347,
         ""
        ],
        [
         "91dd352f-0526-4d0b-ae3e-550765ab0f3c",
         "5c85f060-baa1-496b-a006-ea5bac4e00fe",
         "conn_postgres_chinook",
         "conn_postgres_chinook_Genre",
         "Genre",
         "2025-08-04 11:50:27",
         "overwrite",
         "COMPLETED",
         "None",
         "default.stg_Genre",
         25,
         ""
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "transaction_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "run_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "connection_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "table_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "table_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "dateandtime",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "mode",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "status",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "error",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "target_table",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "number_of_rows",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "other_comments",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 18
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "transaction_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "run_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "connection_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dateandtime",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "mode",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "error",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "target_table",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "number_of_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "other_comments",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select etl_run_logs.* from run_metadata\n",
    "join etl_run_logs on etl_run_logs.run_id=run_metadata.run_id\n",
    "where run_metadata.run_id='5c85f060-baa1-496b-a006-ea5bac4e00fe'\n",
    "--and status = 'COMPLETED'\n",
    "--and error != 'None'\n",
    "--and status='COMPLETED'\n",
    "--and table_name='albums'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7197583550155944,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "Setup_all_with_connection_config",
   "widgets": {
    "connection_id": {
     "currentValue": "",
     "nuid": "a8e9d4be-a056-4df1-97d3-2873881a66c7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Connection ID",
      "name": "connection_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Connection ID",
      "name": "connection_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "cron_expression": {
     "currentValue": "0 3 * * *",
     "nuid": "82281b83-5478-4e2e-a563-15ed847f36a3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "cron_expression",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "cron_expression",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "database": {
     "currentValue": "",
     "nuid": "5620e414-e44c-4f85-b147-74d12749521e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "database",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "database",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "host": {
     "currentValue": "",
     "nuid": "641e2a47-0066-437c-9030-43c74bac6b9e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "host",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "host",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "options": {
     "currentValue": "",
     "nuid": "e77c6c4c-a25f-4c58-9f28-e32d7a109c4b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "options",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "options",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "password": {
     "currentValue": "",
     "nuid": "e4a72449-a617-4df8-9253-b40da913432f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "password",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "password",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "port": {
     "currentValue": "",
     "nuid": "2e6588b5-36a6-46fd-8d95-da3ecfdb5693",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Port (integer)",
      "name": "port",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Port (integer)",
      "name": "port",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "sakila",
     "nuid": "b335bee8-c861-4d02-828f-f2393c09d01b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "",
     "nuid": "6c77accd-a254-42b7-8579-2a54f3e75e05",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "table_id": {
     "currentValue": "",
     "nuid": "13367a74-817c-47a3-a285-1754570b5fec",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Table ID (optional, for single table)",
      "name": "table_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Table ID (optional, for single table)",
      "name": "table_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_account": {
     "currentValue": "",
     "nuid": "6be8bab6-5544-49b0-9174-eb6b66cdc495",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Target Account (for abfss)",
      "name": "target_account",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Target Account (for abfss)",
      "name": "target_account",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_bucket": {
     "currentValue": "",
     "nuid": "69d2e399-df59-4c68-9de2-1d9dffcdeaf9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Target Bucket (for s3)",
      "name": "target_bucket",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Target Bucket (for s3)",
      "name": "target_bucket",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_container": {
     "currentValue": "",
     "nuid": "895048b1-56ea-4cda-800c-12fd08aafa8e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Target Container (for abfss)",
      "name": "target_container",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Target Container (for abfss)",
      "name": "target_container",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_db": {
     "currentValue": "",
     "nuid": "b7621e55-6c5e-4f4d-a601-7b46ef48f386",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Target Databricks,abfss,s3,dbfs Database",
      "name": "target_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Target Databricks,abfss,s3,dbfs Database",
      "name": "target_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_mount": {
     "currentValue": "",
     "nuid": "8ff8f359-d540-42dd-8bfc-2bc7858f0b7d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Target Mount (for dbfs)",
      "name": "target_mount",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Target Mount (for dbfs)",
      "name": "target_mount",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_path_type": {
     "currentValue": "",
     "nuid": "1b7d5c5e-1242-4faa-ae8f-c531faf5cfa1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Target Path Type (optional)",
      "name": "target_path_type",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Target Path Type (optional)",
      "name": "target_path_type",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_prefix": {
     "currentValue": "",
     "nuid": "f39ae39a-a516-4d6b-b122-0528103e7b45",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Target Table Prefix (optional)",
      "name": "target_prefix",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Target Table Prefix (optional)",
      "name": "target_prefix",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_suffix": {
     "currentValue": "",
     "nuid": "72fd16d7-dc6b-4fe4-986f-4f5158bc9d72",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Target Table Suffix (optional)",
      "name": "target_suffix",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Target Table Suffix (optional)",
      "name": "target_suffix",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "type": {
     "currentValue": "mysql",
     "nuid": "ab9f2c84-45c9-4787-9403-9f1bbeed11b5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "type",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "type",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "username": {
     "currentValue": "avnadmin",
     "nuid": "5b74f0af-1ef4-4743-8c2c-543acc23c59a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "username",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "username",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}