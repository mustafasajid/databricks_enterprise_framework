{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4522425f-ad91-466c-9121-9c48e102b670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook Description\n",
    "\n",
    "This notebook is designed to onboard and manage metadata for external database tables in Databricks. It provides a workflow to select tables for onboarding, retrieve their metadata, and prepare JDBC connection properties for various RDBMS types.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Parameter Input via Widgets**:  \n",
    "   Users specify the target table IDs, run ID (optional), and schema using Databricks widgets.\n",
    "\n",
    "2. **Metadata Management**:  \n",
    "   The notebook loads metadata from three tables: `connection_metadata`, `table_metadata`, and `column_metadata`.  \n",
    "   - `connection_metadata`: Contains connection details for various databases.\n",
    "   - `table_metadata`: Contains metadata for tables, including onboarding status.\n",
    "   - `column_metadata`: Contains column-level metadata.\n",
    "\n",
    "3. **Table Selection**:  \n",
    "   The notebook filters tables based on user input and onboarding status, ensuring only eligible tables are processed.\n",
    "\n",
    "4. **Connection Metadata Retrieval**:  \n",
    "   For each selected table, the corresponding connection metadata is retrieved.\n",
    "\n",
    "5. **JDBC Driver Installation**:  \n",
    "   The notebook dynamically installs the required JDBC driver for the detected RDBMS type using `%pip`.\n",
    "\n",
    "6. **JDBC URL and Properties Construction**:  \n",
    "   It constructs the appropriate JDBC URL and connection properties for the target database, supporting multiple RDBMS types (Oracle, SQL Server, PostgreSQL, MySQL, MariaDB, Snowflake, BigQuery, Redshift, DB2, SAP HANA).\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Dynamic RDBMS Support**:  \n",
    "  The notebook supports a wide range of RDBMS platforms, automatically handling driver installation and connection string formatting.\n",
    "\n",
    "- **Metadata-Driven**:  \n",
    "  All operations are driven by metadata tables, enabling flexible and scalable onboarding workflows.\n",
    "\n",
    "- **Widget-Driven Parameterization**:  \n",
    "  Users can easily control the onboarding process through Databricks widgets.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Set the `table_ids` widget to a comma-separated list of table IDs to onboard.\n",
    "2. Optionally set the `run_id` widget, or leave blank to auto-generate.\n",
    "3. Set the `schema` widget as needed.\n",
    "4. Run the notebook cells sequentially.\n",
    "\n",
    "This notebook is intended for data engineers and platform administrators responsible for onboarding and managing external data sources in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "877216a2-3766-45f8-8850-c4b5baed4428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_description = \"\"\"\n",
    "# Notebook Description\n",
    "\n",
    "This notebook is designed to onboard and manage metadata for external database tables in Databricks. It provides a workflow to select tables for onboarding, retrieve their metadata, and prepare JDBC connection properties for various RDBMS types.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Parameter Input via Widgets**:  \n",
    "   Users specify the target table IDs, run ID (optional), and schema using Databricks widgets.\n",
    "\n",
    "2. **Metadata Management**:  \n",
    "   The notebook loads metadata from three tables: `connection_metadata`, `table_metadata`, and `column_metadata`.  \n",
    "   - `connection_metadata`: Contains connection details for various databases.\n",
    "   - `table_metadata`: Contains metadata for tables, including onboarding status.\n",
    "   - `column_metadata`: Contains column-level metadata.\n",
    "\n",
    "3. **Table Selection**:  \n",
    "   The notebook filters tables based on user input and onboarding status, ensuring only eligible tables are processed.\n",
    "\n",
    "4. **Connection Metadata Retrieval**:  \n",
    "   For each selected table, the corresponding connection metadata is retrieved.\n",
    "\n",
    "5. **JDBC Driver Installation**:  \n",
    "   The notebook dynamically installs the required JDBC driver for the detected RDBMS type using `%pip`.\n",
    "\n",
    "6. **JDBC URL and Properties Construction**:  \n",
    "   It constructs the appropriate JDBC URL and connection properties for the target database, supporting multiple RDBMS types (Oracle, SQL Server, PostgreSQL, MySQL, MariaDB, Snowflake, BigQuery, Redshift, DB2, SAP HANA).\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Dynamic RDBMS Support**:  \n",
    "  The notebook supports a wide range of RDBMS platforms, automatically handling driver installation and connection string formatting.\n",
    "\n",
    "- **Metadata-Driven**:  \n",
    "  All operations are driven by metadata tables, enabling flexible and scalable onboarding workflows.\n",
    "\n",
    "- **Widget-Driven Parameterization**:  \n",
    "  Users can easily control the onboarding process through Databricks widgets.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Set the `table_ids` widget to a comma-separated list of table IDs to onboard.\n",
    "2. Optionally set the `run_id` widget, or leave blank to auto-generate.\n",
    "3. Set the `schema` widget as needed.\n",
    "4. Run the notebook cells sequentially.\n",
    "\n",
    "This notebook is intended for data engineers and platform administrators responsible for onboarding and managing external data sources in Databricks.\n",
    "\"\"\"\n",
    "displayHTML(f\"<div style='white-space: pre-wrap'>{notebook_description}</div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6062020-1f71-4ace-8a85-4dd97099a41d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c70cf81-bd0a-4f95-b515-f2af4d0fe1a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks widgets for table_ids and run_id\n",
    "dbutils.widgets.text(\"table_ids\", \"\", \"Table IDs (comma-separated)\")\n",
    "dbutils.widgets.text(\"run_id\", \"\", \"Run ID (leave blank to auto-generate)\")\n",
    "dbutils.widgets.text(\"schema\", \"\")\n",
    "table_ids_param = dbutils.widgets.get(\"table_ids\")\n",
    "run_id_param = dbutils.widgets.get(\"run_id\")\n",
    "table_ids = [tid.strip() for tid in table_ids_param.split(\",\") if tid.strip()]\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "run_id = run_id_param if run_id_param else str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e9345b-cd63-45e8-979d-2b019e15bd91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MetadataManager:\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "\n",
    "    def load_connection_metadata(self) -> DataFrame:\n",
    "        return self.spark.table('connection_metadata')\n",
    "\n",
    "    def load_table_metadata(self) -> DataFrame:\n",
    "        return self.spark.table('table_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258cdc5b-58d5-4bb1-97aa-60cd29c6aa12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "metadata = MetadataManager(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f26f876-a7c3-4f6b-ab5f-97233c08a8c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load metadata tables\n",
    "table_meta = metadata.load_table_metadata()\n",
    "conn_meta = metadata.load_connection_metadata()\n",
    "col_meta = metadata.spark.table('column_metadata')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e1aecd-d8df-4210-ac64-86b6ff3966f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "selected_tables = table_meta.withColumn(\n",
    "    \"onboarded_flag\",\n",
    "    when(col(\"onboarded_flag\").isNull(), \"Y\").otherwise(col(\"onboarded_flag\"))\n",
    ").filter(\n",
    "    (col(\"table_id\").isin(table_ids)) & (col(\"onboarded_flag\") != 'N')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f7a2851-24b4-41a2-8461-e8ee71e34da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(table_id='conn_postgres_chinook_Customer', connection_id='conn_postgres_chinook', table_name='Customer', target_table_name='stg_Customer', table_type='full', primary_key_columns='None', watermark_column='None', partition_column='None', target_path='/mnt/datalake/stg_Customer', load_frequency='daily', active_flag='Y', comments='Auto-populated', optimize_zorder_by='None', repartition_columns='None', num_output_files='None', write_mode='overwrite', cache_intermediate='False', target_db='default', onboarded_flag='Y', table_call_name='\"public\".\"Customer\"')]\n"
     ]
    }
   ],
   "source": [
    "print(selected_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "408c5302-9e2d-4ff8-9c24-710cbe0e71f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(connection_id='conn_postgres_chinook', type='postgresql', host='ep-sweet-snow-aeztchbb-pooler.c-2.us-east-2.aws.neon.tech', port=5432, database='chinook', schema='public', username='neondb_owner', password='npg_7Bd4JRTiqnox', options='')\n"
     ]
    }
   ],
   "source": [
    "for row in selected_tables:\n",
    "    conn = conn_meta.filter(conn_meta.connection_id == row.connection_id).collect()[0]\n",
    "    print(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5040f9-c1c2-4a74-86d3-cb938a2d26a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /local_disk0/.ephemeral_nfs/envs/pythonEnv-214f3ceb-ecee-48c5-b236-fd72c15cb887/lib/python3.12/site-packages (2.9.10)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "if conn.type == \"oracle\":\n",
    "    %pip install cx_Oracle\n",
    "elif conn.type == \"sqlserver\":\n",
    "    %pip install pyodbc\n",
    "elif conn.type == \"postgresql\":\n",
    "    %pip install psycopg2-binary\n",
    "elif conn.type == \"mysql\":\n",
    "    %pip install mysql-connector-python\n",
    "elif conn.type == \"mariadb\":\n",
    "    %pip install mariadb\n",
    "elif conn.type == \"snowflake\":\n",
    "    %pip install snowflake-connector-python\n",
    "elif conn.type == \"bigquery\":\n",
    "    %pip install google-cloud-bigquery\n",
    "elif conn.type == \"redshift\":\n",
    "    %pip install redshift-connector\n",
    "elif conn.type == \"db2\":\n",
    "    %pip install ibm-db\n",
    "elif conn.type == \"hana\":\n",
    "    %pip install hdbcli\n",
    "else:\n",
    "    raise Exception(f\"Unsupported RDBMS type: {conn.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdeb80d0-eb5e-4631-814a-539980474a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbc_url = None\n",
    "connection_properties = {}\n",
    "\n",
    "if conn.type == \"oracle\":\n",
    "    jdbc_url = f\"jdbc:oracle:thin:@{conn.host}:{conn.port}/{conn.database}\"\n",
    "    connection_properties = {\n",
    "        \"user\": conn.username,\n",
    "        \"password\": conn.password,\n",
    "        \"driver\": \"oracle.jdbc.OracleDriver\",\n",
    "        \"currentSchema\": conn.schema\n",
    "    }\n",
    "elif conn.type == \"sqlserver\":\n",
    "    jdbc_url = f\"jdbc:sqlserver://{conn.host}:{conn.port};databaseName={conn.database};schema={conn.schema}\"\n",
    "    connection_properties = {\n",
    "        \"user\": conn.username,\n",
    "        \"password\": conn.password,\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "        \"schema\": conn.schema\n",
    "    }\n",
    "elif conn.type == \"postgresql\":\n",
    "    jdbc_url = f\"jdbc:postgresql://{conn.host}:{conn.port}/{conn.database}\"\n",
    "    connection_properties = {\n",
    "        \"user\": conn.username,\n",
    "        \"password\": conn.password,\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"currentSchema\": conn.schema\n",
    "    }\n",
    "elif conn.type == \"mysql\":\n",
    "    jdbc_url = f\"jdbc:mysql://{conn.host}:{conn.port}/{conn.database}\"\n",
    "    connection_properties = {\n",
    "        \"user\": conn.username,\n",
    "        \"password\": conn.password,\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "        \"schema\": conn.schema\n",
    "    }\n",
    "elif conn.type == \"mariadb\":\n",
    "    jdbc_url = f\"jdbc:mariadb://{conn.host}:{conn.port}/{conn.database}\"\n",
    "    connection_properties = {\n",
    "        \"user\": conn.username,\n",
    "        \"password\": conn.password,\n",
    "        \"driver\": \"org.mariadb.jdbc.Driver\",\n",
    "        \"schema\": conn.schema\n",
    "    }\n",
    "elif conn.type == \"snowflake\":\n",
    "    jdbc_url = f\"jdbc:snowflake://{conn.host}/?db={conn.database}&schema={conn.schema}\"\n",
    "    connection_properties = {\n",
    "        \"user\": conn.username,\n",
    "        \"password\": conn.password,\n",
    "        \"driver\": \"net.snowflake.client.jdbc.SnowflakeDriver\",\n",
    "        \"schema\": conn.schema\n",
    "    }\n",
    "elif conn.type == \"bigquery\":\n",
    "    jdbc_url = f\"jdbc:bigquery://https://www.googleapis.com/bigquery/v2:443;ProjectId={conn.database};DefaultDataset={conn.schema};\"\n",
    "    connection_properties = {\n",
    "        \"user\": conn.username,\n",
    "        \"password\": conn.password,\n",
    "        \"driver\": \"com.simba.googlebigquery.jdbc42.Driver\",\n",
    "        \"DefaultDataset\": conn.schema\n",
    "    }\n",
    "elif conn.type == \"redshift\":\n",
    "    jdbc_url = f\"jdbc:redshift://{conn.host}:{conn.port}/{conn.database}\"\n",
    "    connection_properties = {\n",
    "        \"user\": conn.username,\n",
    "        \"password\": conn.password,\n",
    "        \"driver\": \"com.amazon.redshift.jdbc.Driver\",\n",
    "        \"currentSchema\": conn.schema\n",
    "    }\n",
    "elif conn.type == \"db2\":\n",
    "    jdbc_url = f\"jdbc:db2://{conn.host}:{conn.port}/{conn.database}\"\n",
    "    connection_properties = {\n",
    "        \"user\": conn.username,\n",
    "        \"password\": conn.password,\n",
    "        \"driver\": \"com.ibm.db2.jcc.DB2Driver\",\n",
    "        \"currentSchema\": conn.schema\n",
    "    }\n",
    "elif conn.type == \"hana\":\n",
    "    jdbc_url = f\"jdbc:sap://{conn.host}:{conn.port}\"\n",
    "    connection_properties = {\n",
    "        \"user\": conn.username,\n",
    "        \"password\": conn.password,\n",
    "        \"driver\": \"com.sap.db.jdbc.Driver\",\n",
    "        \"currentSchema\": conn.schema\n",
    "    }\n",
    "else:\n",
    "    raise Exception(f\"Unsupported RDBMS type: {conn.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f47ba7d-caad-49c2-ab6d-e5ee0bb03ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_delta(df, mode, target_table, run_id, connection_id, table_id, table_name):\n",
    "    from datetime import datetime\n",
    "\n",
    "    log_table = \"workspace.default.etl_run_logs\"\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    transaction_id = str(uuid.uuid4())\n",
    "    #print(\"all good\")\n",
    "    # Log INPROGRESS\n",
    "    inprogress_log = [(transaction_id, run_id, connection_id, table_id, table_name, now, mode, \"INPROGRESS\", 'None', target_table, 0, \"\")]\n",
    "    inprogress_df = spark.createDataFrame(\n",
    "        inprogress_log,\n",
    "        [\"transaction_id\", \"run_id\", \"connection_id\", \"table_id\", \"table_name\", \"dateandtime\", \"mode\", \"status\", \"error\", \"target_table\", \"number_of_rows\", \"other_comments\"]\n",
    "    )\n",
    "    inprogress_df.write.format(\"delta\").mode(\"append\").saveAsTable(log_table)\n",
    "\n",
    "    error = 'None'\n",
    "    number_of_rows = 0\n",
    "    try:\n",
    "        df.write.format(\"delta\").mode(mode).saveAsTable(target_table)\n",
    "        number_of_rows = df.count()\n",
    "        # Log COMPLETED\n",
    "        completed_log = [(transaction_id, run_id, connection_id, table_id, table_name, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), mode, \"COMPLETED\", 'None', target_table, number_of_rows, \"\")]\n",
    "        completed_df = spark.createDataFrame(\n",
    "            completed_log,\n",
    "            [\"transaction_id\", \"run_id\", \"connection_id\", \"table_id\", \"table_name\", \"dateandtime\", \"mode\", \"status\", \"error\", \"target_table\", \"number_of_rows\", \"other_comments\"]\n",
    "        )\n",
    "        completed_df.write.format(\"delta\").mode(\"append\").saveAsTable(log_table)\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        # Log ERROR\n",
    "        error_log = [(transaction_id, run_id, connection_id, table_id, table_name, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), mode, \"ERROR\", error, target_table, number_of_rows, \"\")]\n",
    "        error_df = spark.createDataFrame(\n",
    "            error_log,\n",
    "            [\"transaction_id\", \"run_id\", \"connection_id\", \"table_id\", \"table_name\", \"dateandtime\", \"mode\", \"status\", \"error\", \"target_table\", \"number_of_rows\", \"other_comments\"]\n",
    "        )\n",
    "        error_df.write.format(\"delta\").mode(\"append\").saveAsTable(log_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef9e474-b7a8-41c8-b551-e69df957392d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "\n",
    "\n",
    "for row in selected_tables:\n",
    "    #conn = conn_meta.filter(conn_meta.connection_id == row.connection_id).collect()[0]\n",
    "    tbl = row.table_call_name\n",
    "    target_table = f\"{row.target_db}.{row.target_table_name}\" if row.target_db else row.target_table_name\n",
    "    columns = col_meta.filter(col_meta.table_id == row.table_id).collect()\n",
    "    col_map = {col.column_name: col.target_column_name if hasattr(col, \"target_column_name\") and col.target_column_name else col.column_name for col in columns}\n",
    "    def apply_col_mapping(df, col_map):\n",
    "        for src, tgt in col_map.items():\n",
    "            if src != tgt:\n",
    "                df = df.withColumnRenamed(src, tgt)\n",
    "        return df\n",
    "    if row.table_type == \"full\":\n",
    "        df = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=tbl,\n",
    "            properties=connection_properties\n",
    "        )\n",
    "        df = apply_col_mapping(df, col_map)\n",
    "        write_to_delta(df, \"overwrite\", target_table, run_id, row.connection_id, row.table_id, row.table_name)\n",
    "    elif row.table_type == \"incremental\":\n",
    "        try:\n",
    "            target_df = spark.table(target_table)\n",
    "            max_watermark = target_df.agg({row.watermark_column: \"max\"}).collect()[0][0]\n",
    "        except Exception:\n",
    "            max_watermark = None\n",
    "        predicate = f\"{row.watermark_column} > '{max_watermark}'\" if max_watermark else None\n",
    "        if predicate:\n",
    "            connection_properties[\"predicate\"] = predicate\n",
    "        df = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=tbl,\n",
    "            properties=connection_properties\n",
    "        )\n",
    "        df = apply_col_mapping(df, col_map)\n",
    "        write_to_delta(df, \"append\", target_table, run_id, row.connection_id, row.table_id, row.table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5858805587465178,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "metadata_driven_ingestion",
   "widgets": {
    "run_id": {
     "currentValue": "",
     "nuid": "0458e128-b2fd-4e7a-af06-e9d17f8a6090",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Run ID (leave blank to auto-generate)",
      "name": "run_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Run ID (leave blank to auto-generate)",
      "name": "run_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "public",
     "nuid": "c81918fd-e871-4f29-b66a-10b3bacbb384",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_ids": {
     "currentValue": "conn_postgres_chinook_Customer",
     "nuid": "e55910a4-a046-4d70-bf58-1f7d6dc07113",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Table IDs (comma-separated)",
      "name": "table_ids",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Table IDs (comma-separated)",
      "name": "table_ids",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}